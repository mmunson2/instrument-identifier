# instrument-identifier

This project aims to create a system capable of reading instruments from a simulated aircraft cockpit. It will take a video feed from a simulator and apply one or more machine vision processes to read values from a set of target instruments.

## Why?

Pilots in training may want to verify how they are reading an aircraft’s instruments, users of flight simulators might want to record information about their flight but don’t have access to developer APIs, and video streaming services such as Twitch could use this to collect additional information about what a streamer is doing. And it's fun :)

## Release 1: October 16th 2023

This release is intended to be a quick prototype of the entire project end-to-end. To simplify:

- The input video has a fixed perspective - the instruments are therefore always within a certain region
- Only the vertical speed indicator is evaluated

### Extract Frames

This is a simple helper for extracting all of the frames in an input video and cropping them down to a specific region. I use this to generate hundreds of images of instruments for training models.
  
### Label Frames

This is a tool for labelling each of the frames generated by the frame extractor. We have to assign a value to each frame representing the instrument reading. This will be used to train the deep learning model to read instruments.
  
Because the input is a video of an aircraft flying, the changes in instrument reading from frame-to-frame are relatively small. This tool makes it easy to quickly increment or decrement the labelling value by a predefined step, then apply this to any number of frames. It takes about 10 minutes for a person to label 1000 frames, which is enough to train a relatively accurate model.

![Frame labelling example image](demo_screenshots/labelling_tool.png "Frame Labeller Example")
  
TODO:  
- Better support for viewing/editing frames that have already been edited. 

### Train Deep Learning Model

This tool uses tensorflow to train a model from a directory of training frames and a CSV of labels. The hyperparameters for the model are relatively standard in this verison, they can be further improved in later releases.
  
Current accuracy for vertical speed readings is 90%.  
  
TODO:  
- Adjust hyperparameters and attempt to improve accuracy

### Evaluate Deep Learning Model

To provide a visual output of the system, this tool displays frames of the input video along with a scaled-up image of the vertical speed indicator and the model's interpretation of its value. This allows a user to identify patterns in the model's accuracy, as well as providing a "real-life" example of the system working.

![Output evaluation demonstration image](demo_screenshots/Output_View.png "Output example frame")

TODO:
- Provide a visual representation of the aircraft and its attitude/speed/etc based on the model's interpretation of the instrument(s).






